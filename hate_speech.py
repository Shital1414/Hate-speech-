# -*- coding: utf-8 -*-
"""Hate speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JRt1r3VVVA2c1szSrcq1YX3NVN9mNNOk
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/hateXplain.csv')

# Display the first few rows of the dataframe
display(df.head())

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data (if not already downloaded)
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize the WordNet Lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Handle textual noise: URLs, user mentions, and special characters
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE) # Remove URLs
    text = re.sub(r'@\w+', '', text) # Remove user mentions
    text = re.sub(r'[^A-Za-z0-9 ]+', '', text) # Remove special characters

    # Normalize text: convert to lowercase
    text = text.lower()

    # Tokenize the text
    tokens = text.split()

    # Remove stop words and apply lemmatization
    processed_tokens = []
    for word in tokens:
        if word not in stop_words:
            processed_tokens.append(lemmatizer.lemmatize(word))

    return ' '.join(processed_tokens)

# Apply the preprocessing function to the 'post_tokens' column
df['cleaned_post_tokens'] = df['post_tokens'].apply(preprocess_text)

# Display the first few rows with the new cleaned column
display(df[['post_tokens', 'cleaned_post_tokens']].head())

import matplotlib.pyplot as plt
import seaborn as sns

# Analyze the distribution of the 'label' column
plt.figure(figsize=(8, 6))
sns.countplot(x='label', data=df)
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# Display the value counts
display(df['label'].value_counts())


from collections import Counter

def get_most_common_words(df, label, n=20):
    """Gets the most common words for a given label."""
    text = ' '.join(df[df['label'] == label]['cleaned_post_tokens']).split()
    return Counter(text).most_common(n)

# Get most common words for each label
normal_words = get_most_common_words(df, 'normal')
hatespeech_words = get_most_common_words(df, 'hatespeech')
offensive_words = get_most_common_words(df, 'offensive')

print("Most common words in 'normal' posts:")
display(normal_words)

print("\nMost common words in 'hatespeech' posts:")
display(hatespeech_words)

print("\nMost common words in 'offensive' posts:")
display(offensive_words)

# Calculate the length of cleaned post tokens
df['cleaned_post_length'] = df['cleaned_post_tokens'].apply(lambda x: len(x.split()))

# Analyze the distribution of post lengths for each label
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='cleaned_post_length', hue='label', multiple='stack', bins=50)
plt.title('Distribution of Cleaned Post Lengths by Label')
plt.xlabel('Cleaned Post Length')
plt.ylabel('Count')
plt.show()

# Display descriptive statistics for post lengths by label
display(df.groupby('label')['cleaned_post_length'].describe())

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
# We can limit the number of features to avoid a very large matrix
tfidf_vectorizer = TfidfVectorizer(max_features=5000)

# Fit and transform the cleaned post tokens
tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_post_tokens'])

# Display the shape of the TF-IDF matrix
print("Shape of TF-IDF matrix:", tfidf_matrix.shape)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['label'], test_size=0.25, random_state=42)

# Initialize and train a Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
display(confusion_matrix(y_test, y_pred))



#Model evaluation and improvement
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l2'] # Using only l2 penalty as l1 might require different solver
}

# Initialize GridSearchCV
grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1_weighted')

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred_tuned = best_model.predict(X_test)

# Evaluate the best model
print("\nClassification Report (Tuned Model):")
print(classification_report(y_test, y_pred_tuned))

print("\nConfusion Matrix (Tuned Model):")
display(confusion_matrix(y_test, y_pred_tuned))

#Explore more sophisticated models
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

# Initialize a MultinomialNB model
naive_bayes_model = MultinomialNB()

# Train the MultinomialNB model
naive_bayes_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_nb = naive_bayes_model.predict(X_test)

# Evaluate the MultinomialNB model
print("Classification Report (Naive Bayes):")
print(classification_report(y_test, y_pred_nb))

print("\nConfusion Matrix (Naive Bayes):")
display(confusion_matrix(y_test, y_pred_nb))

# Comprehensive model evaluation
print("Comparison of Model Performance:")
print("\nLogistic Regression (Tuned) - Classification Report:")
print(classification_report(y_test, y_pred_tuned))

print("\nLogistic Regression (Tuned) - Confusion Matrix:")
display(confusion_matrix(y_test, y_pred_tuned))

print("\nMultinomial Naive Bayes - Classification Report:")
print(classification_report(y_test, y_pred_nb))

print("\nMultinomial Naive Bayes - Confusion Matrix:")
display(confusion_matrix(y_test, y_pred_nb))

print("\n--- Comparative Analysis ---")
print("Overall Accuracy:")
print(f"Logistic Regression (Tuned): {best_model.score(X_test, y_test):.2f}")
print(f"Multinomial Naive Bayes: {naive_bayes_model.score(X_test, y_test):.2f}")

print("\nStrengths and Weaknesses based on Precision, Recall, and F1-Score:")

# Analyze Logistic Regression
print("\nLogistic Regression (Tuned) Analysis:")
lr_report = classification_report(y_test, y_pred_tuned, output_dict=True)
print(f"- 'normal' class: Precision={lr_report['normal']['precision']:.2f}, Recall={lr_report['normal']['recall']:.2f}, F1-Score={lr_report['normal']['f1-score']:.2f}")
print(f"- 'hatespeech' class: Precision={lr_report['hatespeech']['precision']:.2f}, Recall={lr_report['hatespeech']['recall']:.2f}, F1-Score={lr_report['hatespeech']['f1-score']:.2f}")
print(f"- 'offensive' class: Precision={lr_report['offensive']['precision']:.2f}, Recall={lr_report['offensive']['recall']:.2f}, F1-Score={lr_report['offensive']['f1-score']:.2f}")
print("  - Strengths: Good recall for 'normal' class. Decent overall balance for 'hatespeech'.")
print("  - Weaknesses: Lower precision for 'normal' and 'offensive' classes, indicating more false positives. Struggles with 'offensive' class overall.")


# Analyze Multinomial Naive Bayes
print("\nMultinomial Naive Bayes Analysis:")
nb_report = classification_report(y_test, y_pred_nb, output_dict=True)
print(f"- 'normal' class: Precision={nb_report['normal']['precision']:.2f}, Recall={nb_report['normal']['recall']:.2f}, F1-Score={nb_report['normal']['f1-score']:.2f}")
print(f"- 'hatespeech' class: Precision={nb_report['hatespeech']['precision']:.2f}, Recall={nb_report['hatespeech']['recall']:.2f}, F1-Score={nb_report['hatespeech']['f1-score']:.2f}")
print(f"- 'offensive' class: Precision={nb_report['offensive']['precision']:.2f}, Recall={nb_report['offensive']['recall']:.2f}, F1-Score={nb_report['offensive']['f1-score']:.2f}")
print("  - Strengths: Decent precision and recall for 'hatespeech' class.")
print("  - Weaknesses: Lower precision for 'normal' and 'offensive'. Struggles with 'offensive' recall.")

print("\nOverall Comparison:")
print("- The tuned Logistic Regression model shows slightly better overall accuracy and weighted average F1-score compared to Multinomial Naive Bayes.")
print("- For identifying 'hatespeech', both models have similar precision and recall, with Logistic Regression having a slightly higher F1-score.")
print("- Logistic Regression is better at identifying 'normal' posts (higher recall), but with lower precision (more false positives).")
print("- Both models struggle with the 'offensive' class, showing lower precision and recall compared to the other classes. Logistic Regression has a slightly better F1-score for this class.")
print("- Based on the weighted average F1-score, the tuned Logistic Regression model appears to be slightly better overall for this dataset.")



#Reasoning: Review the notebook, add markdown cells for explanations, create the README.md content, and save the best model.

import joblib
import os

# Add markdown cells for explanations in the notebook
# This part is done manually in the notebook interface by adding new markdown cells and editing existing ones.
# The markdown cells should cover:
# 1. Project Title and Goal
# 2. Data Loading and Initial Exploration
# 3. Data Preprocessing (Explanation of steps: cleaning, normalization, tokenization, stop words, lemmatization)
# 4. Exploratory Data Analysis (EDA) (Explanation of label distribution and post length analysis)
# 5. Feature Engineering (Explanation of TF-IDF)
# 6. Model Training and Evaluation (Explanation of models used, evaluation metrics, and results)
# 7. Model Selection (Justification for choosing the best model)

# Create the content for the README.md file
readme_content = """# Hate Speech and Offensive Language Detection

## Project Goal
The goal of this project is to build and evaluate machine learning models to detect hate speech and offensive language in text data.

## Dataset
The project uses the `hateXplain.csv` dataset, which contains text posts and corresponding labels ('normal', 'hatespeech', 'offensive').

## Methodology
1.  **Data Preprocessing:**
    - Handling textual noise (URLs, mentions, special characters).
    - Normalizing text (lowercase conversion).
    - Tokenization.
    - Removing stop words and applying lemmatization.
2.  **Exploratory Data Analysis (EDA):**
    - Analyzing the distribution of labels.
    - Examining the distribution of post lengths for different labels.
    - Identifying the most common words for each label.
3.  **Feature Engineering:**
    - Using TF-IDF (Term Frequency-Inverse Document Frequency) to convert text data into numerical features.
4.  **Model Training and Evaluation:**
    - Training and evaluating a Logistic Regression model (with hyperparameter tuning using GridSearchCV).
    - Training and evaluating a Multinomial Naive Bayes model.
    - Evaluating models using Accuracy, Precision, Recall, F1-Score, and Confusion Matrix.
5.  **Model Selection:**
    - Comparing the performance of the trained models based on evaluation metrics.

## Key Findings and Model Selection
Based on the comprehensive evaluation, the **Tuned Logistic Regression model** was selected as the best-performing model for this task. It showed a slightly higher overall accuracy and weighted average F1-score compared to the Multinomial Naive Bayes model. The tuned Logistic Regression model also demonstrated better performance in detecting 'hatespeech' and 'offensive' instances, although both models faced challenges with the 'offensive' class.

## How to Run the Notebook
1.  Ensure you have Jupyter Notebook or JupyterLab installed.
2.  Install the required dependencies:
    ```bash
    pip install pandas scikit-learn nltk matplotlib seaborn joblib
    ```
3.  Download the `hateXplain.csv` dataset and place it in the same directory as the notebook, or update the data path in the notebook.
4.  Open the `Hate_Speech_Detection.ipynb` (or similar name) notebook.
5.  Run the cells sequentially to execute the data loading, preprocessing, EDA, feature engineering, model training, and evaluation steps.

## Dependencies
- pandas
- scikit-learn
- nltk
- matplotlib
- seaborn
- joblib

"""

# Save the README.md file
with open("README.md", "w") as f:
    f.write(readme_content)

# Save the best-performing model (tuned Logistic Regression)
# Ensure 'best_model' variable is available from previous steps
if 'best_model' in locals():
    model_filename = 'best_logistic_regression_model.joblib'
    joblib.dump(best_model, model_filename)
    print(f"Best model saved to {model_filename}")
else:
    print("Best model variable not found. Ensure the model training cell was executed.")